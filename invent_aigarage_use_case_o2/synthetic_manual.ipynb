{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>inventor</th>\n",
       "      <th>assignee</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>language</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>pdf</th>\n",
       "      <th>page</th>\n",
       "      <th>entities</th>\n",
       "      <th>text_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Vision system for a vehicle</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Kenneth Schofield</td>\n",
       "      <td>Magna Electronics Inc.</td>\n",
       "      <td>US10427604B2</td>\n",
       "      <td>en</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/24...</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/57...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"technologies\": [\"vision system\"], \"places\": ...</td>\n",
       "      <td>Vision system for a vehicle: Title: Vision Sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Vision system for a vehicle</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>Kenneth Schofield</td>\n",
       "      <td>Magna Electronics Inc.</td>\n",
       "      <td>US10427604B2</td>\n",
       "      <td>en</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/24...</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/57...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vision system for a vehicle: This patent prese...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                        title publication_date           inventor  \\\n",
       "0     0  Vision system for a vehicle       2019-10-01  Kenneth Schofield   \n",
       "1     0  Vision system for a vehicle       2019-10-01  Kenneth Schofield   \n",
       "\n",
       "                 assignee publication_number language  \\\n",
       "0  Magna Electronics Inc.       US10427604B2       en   \n",
       "1  Magna Electronics Inc.       US10427604B2       en   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  https://patentimages.storage.googleapis.com/24...   \n",
       "1  https://patentimages.storage.googleapis.com/24...   \n",
       "\n",
       "                                                 pdf  page  \\\n",
       "0  https://patentimages.storage.googleapis.com/57...   1.0   \n",
       "1  https://patentimages.storage.googleapis.com/57...   1.0   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {\"technologies\": [\"vision system\"], \"places\": ...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                         text_chunks  \n",
       "0  Vision system for a vehicle: Title: Vision Sys...  \n",
       "1  Vision system for a vehicle: This patent prese...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"a4_db_patents.csv\")\n",
    "df1 = df1[['rank', 'title', 'publication_date',\n",
    "        'inventor', 'assignee', 'publication_number', 'language', 'thumbnail', 'pdf', 'page', 'entities', 'text_chunks'\n",
    "        ]]\n",
    "\n",
    "df1.to_csv(\"a4_db_patents.csv\", index=False)\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank', 'title', 'publication_date', 'inventor', 'assignee',\n",
       "       'publication_number', 'language', 'thumbnail', 'pdf', 'page',\n",
       "       'entities', 'text_chunks'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker to generate fake data\n",
    "fake = Faker()\n",
    "\n",
    "titles = [\n",
    "    \"Mastering Your Vehicle's Red Alerts: A Comprehensive Guide to Critical Maintenance\",\n",
    "    \"Navigating Amber Warnings: Essential Maintenance Tips to Keep Your Vehicle Running Smoothly\",\n",
    "    \"Green Light Driving: Optimizing Performance and Efficiency for Your Vehicle\",\n",
    "    \"Understanding Red Signals: Troubleshooting and Resolving Critical Automotive Issues\",\n",
    "    \"Amber Alert: Your Guide to Intermediate Maintenance and Precautionary Measures\",\n",
    "    \"Green Zone Maintenance: Proactive Strategies for Long-Term Vehicle Health\",\n",
    "    \"Red Flag Repairs: Step-by-Step Solutions for Urgent Automotive Problems\",\n",
    "    \"Amber Zone Awareness: Identifying Potential Issues and Taking Preventive Action\",\n",
    "    \"Driving in the Green Lane: Maintaining Peak Performance and Reliability\",\n",
    "    \"Red Alert Resolutions: Swift and Effective Responses to Emergency Automotive Situations\"\n",
    "]\n",
    "\n",
    "\n",
    "# Number of entries\n",
    "num_entries = 10\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'rank': np.arange(1, num_entries + 1),\n",
    "    'title': titles,\n",
    "    'publication_date': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_entries)],\n",
    "    'inventor': [fake.name() for _ in range(num_entries)],\n",
    "    'assignee': [fake.company() for _ in range(num_entries)],\n",
    "    'publication_number': [str(fake.random_number(digits=10)) for _ in range(num_entries)],\n",
    "    'language': ['en'] * num_entries,\n",
    "    'thumbnail': [fake.image_url() for _ in range(num_entries)],\n",
    "    'pdf': [fake.url() for _ in range(num_entries)],\n",
    "    'page': [random.randint(1, 10) for _ in range(num_entries)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank', 'title', 'publication_date', 'inventor', 'assignee',\n",
       "       'publication_number', 'language', 'thumbnail', 'pdf', 'page'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patent text for ID 1740611947 saved successfully.\n",
      "Patent text for ID 9754827200 saved successfully.\n",
      "Patent text for ID 1640155484 saved successfully.\n",
      "Patent text for ID 6773150709 saved successfully.\n",
      "Patent text for ID 1877850619 saved successfully.\n",
      "Patent text for ID 2361427102 saved successfully.\n",
      "Patent text for ID 7395006099 saved successfully.\n",
      "Patent text for ID 8632135679 saved successfully.\n",
      "Patent text for ID 5765289575 saved successfully.\n",
      "Patent text for ID 8281010559 saved successfully.\n",
      "All patent texts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Function to generate synthetic patent text\n",
    "def generate_patent_text(title):\n",
    "    prompt = f\"Create a text to simulate a manual on: \\\"{title}\\\", related to the automotive industry\"\n",
    "    deployment_name = \"gpt-35-turbo-instruct\"\n",
    "    response = client.completions.create(\n",
    "        model=deployment_name,\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Function to save synthetic patent text to file\n",
    "def save_patent_text_to_file(id, text):\n",
    "    directory = \"synthetic_manuals\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(os.path.join(directory, f\"{id.replace(\"/\", \"_\")}.txt\"), \"w\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Generate synthetic patent texts and save them to files\n",
    "for index, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    patent_text = generate_patent_text(title)\n",
    "    save_patent_text_to_file(row['publication_number'], patent_text)\n",
    "    print(f\"Patent text for ID {row['publication_number']} saved successfully.\")\n",
    "\n",
    "print(\"All patent texts saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store text data\n",
    "text_data = []\n",
    "\n",
    "\n",
    "# Function to read text files and return text content\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "    \n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Read the corresponding text file\n",
    "    file_path = f\"synthetic_manuals/{row['publication_number'].replace(\"/\", \"_\")}.txt\"\n",
    "    if os.path.exists(file_path):\n",
    "        text_content = read_text_file(file_path)\n",
    "        text_data.append({\"publication_number\": row[\"publication_number\"], \"text\": text_content})\n",
    "\n",
    "# Create a DataFrame from the text data\n",
    "text_df = pd.DataFrame(text_data)\n",
    "\n",
    "# Join the text DataFrame with the original DataFrame on the \"id\" column\n",
    "merged_df = pd.merge(df, text_df, on=\"publication_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_char = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    separators=[\"<ele>\", \". \", \" \"],\n",
    "    keep_separator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def chunk_documents(content_df: pd.DataFrame, text_splitter, word_count_threshold: int = 10, element_separator: str = \"<ele>\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into chunks according to the configuration of the text splitter\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Split content into chunks\n",
    "    content_df[\"text_chunks\"] = content_df[\"text\"].apply(text_splitter.split_text)\n",
    "    content_df = content_df.explode(\"text_chunks\")\n",
    "    content_df[\"text_chunks\"] = content_df[\"text_chunks\"].str.replace(element_separator, \", \")\n",
    "    \n",
    "    content_df[\"text_chunks\"] = content_df[\"title\"] + \": \" + content_df[\"text_chunks\"]\n",
    "    \n",
    "    # Drop failed extractions\n",
    "    print(f'Files with extraction errors: {set(content_df[content_df[\"text_chunks\"].isna()][\"title\"].to_list())}')\n",
    "    content_df = content_df.dropna(axis=0, subset=[\"text_chunks\"])\n",
    "    \n",
    "    # Drop chunks with less words than the threshold allows\n",
    "    content_df = content_df[content_df[\"text_chunks\"].apply(lambda n: len(n.split())) > word_count_threshold]\n",
    "\n",
    "    return content_df\n",
    "\n",
    "# Example usage:\n",
    "# chunked_df = chunk_documents(df, text_splitter_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with extraction errors: set()\n"
     ]
    }
   ],
   "source": [
    "chunk_df = chunk_documents(merged_df, text_splitter_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Few-shot prompt for entity extraction\n",
    "few_shot_prompt = \"\"\"RETURN ONLY A VALID JSON OBJECT!!! NO ADDITIONAL TEXT\n",
    "\n",
    "Extract relevant entities such as people, technologies, places, and organizations from the given text.\n",
    "\n",
    "Example 1:\n",
    "Text: \"The invention relates to an advanced LiDAR technology for environmental perception.\"\n",
    "{\"technologies\": [\"LiDAR\"], \"places\": [], \"people\": [], \"organizations\": []}\n",
    "\n",
    "\n",
    "Example 2:\n",
    "Text: \"The vehicle was tested in urban environments such as New York City and Los Angeles.\"\n",
    "{\"technologies\": [], \"places\": [\"New York City\", \"Los Angeles\"], \"people\": [], \"organizations\": []}\n",
    "\n",
    "Example 3:\n",
    "Text: \"The engineers John Smith and Alice Johnson collaborated on the development of the navigation system.\"\n",
    "{\"technologies\": [\"navigation system\"], \"places\": [], \"people\": [\"John Smith\", \"Alice Johnson\"], \"organizations\": []}\n",
    "\n",
    "Example 4:\n",
    "Text: \"The company XYZ Inc. funded the research project on autonomous vehicle navigation.\"\n",
    "{\"technologies\": [\"autonomous vehicle navigation\"], \"places\": [], \"people\": [], \"organizations\": [\"XYZ Inc.\"]}\n",
    "\n",
    "Example 4:\n",
    "Text: \"A glass filled with liquids.\"\n",
    "{\"technologies\": [], \"places\": [], \"people\": [], \"organizations\": []}\n",
    "\n",
    "\n",
    "RETURN ONLY A VALID JSON OBJECT!!! NO ADDITIOANL TEXT. JUST A JSON\n",
    "IF THERE ARE NOT ENTITIES RETURN AN EMPTY JSON\n",
    "---\"\"\"\n",
    "\n",
    "\n",
    "# Function to extract entities from text using Azure OpenAI API\n",
    "def extract_entities(text):\n",
    "    prompt = f\"{few_shot_prompt}\\nText: \\\"{text}\\\"\\n\"\n",
    "    deployment_name = \"gpt-35-turbo-instruct\"\n",
    "    response = client.completions.create(\n",
    "        model=deployment_name,\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    # Parse the response as JSON\n",
    "    entities_json = response.choices[0].text.strip()\n",
    "    return entities_json\n",
    "\n",
    "\n",
    "# Create an empty list to store extracted entity data\n",
    "entity_data = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in chunk_df.iterrows():\n",
    "    text = row[\"text_chunks\"]\n",
    "    # Extract entities from the text\n",
    "    entities = extract_entities(text)\n",
    "    if entities is not None:\n",
    "        entity_data.append(entities)\n",
    "    else:\n",
    "        entity_data.append('')\n",
    "\n",
    "\n",
    "chunk_df[\"entities\"] = entity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retriever_model_inference(text_input: list[str]):\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=\"2024-03-01-preview\"\n",
    "        )\n",
    "\n",
    "    embeddings = client.embeddings.create(\n",
    "        input=text_input,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    vectors = [np.array(i.embedding, dtype='f') for i in embeddings.data]\n",
    "\n",
    "\n",
    "    return vectors\n",
    "\n",
    "\n",
    "chunk_df[\"embeddings\"] = retriever_model_inference(chunk_df.text_chunks.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "vector_dim=1536\n",
    "partition_nr=10\n",
    "\n",
    "from utils.faiss_storage import train_index, add_index_vectors\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. initialize index\n",
    "#index = index_initializaton(vector_dim, partition_nr)\n",
    "path_to_index = \"a4_db_data.index\"\n",
    "\n",
    "def vector_db_setup(df: pd.DataFrame):\n",
    "\n",
    "    index = faiss.read_index(path_to_index)\n",
    "\n",
    "    print(index.ntotal)\n",
    "    \n",
    "    vectors = df[\"embeddings\"].values\n",
    "    vectors = np.array([i.astype(np.float32) for i in chunk_df[\"embeddings\"].values])\n",
    "    train_index(index, vectors)\n",
    "    # 3. add vectors to index\n",
    "    add_index_vectors(index, vectors)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "index = vector_db_setup(chunk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, path_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = faiss.read_index(path_to_index)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.drop(columns=[\"embeddings\", \"text\"]).to_csv(\"a4_db_manuals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"a4_db_patents.csv\")\n",
    "manuals = pd.read_csv(\"a4_db_manuals.csv\")\n",
    "\n",
    "\n",
    "pd.concat([data, manuals]).to_csv(\"a4_db_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
