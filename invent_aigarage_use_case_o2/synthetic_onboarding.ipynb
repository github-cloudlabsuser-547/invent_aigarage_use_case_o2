{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    \"Administrative Policies and Procedures\",\n",
    "    \"Organizational Structure\",\n",
    "    \"Employee Handbook\",\n",
    "    \"HR Management Systems\",\n",
    "    \"Onboarding Processes\",\n",
    "    \"New Hire Orientation\",\n",
    "    \"Personnel Files Management\",\n",
    "    \"Compliance Training\",\n",
    "    \"Workplace Safety Procedures\",\n",
    "    \"Performance Evaluation Systems\",\n",
    "    \"Benefits Administration\",\n",
    "    \"Time and Attendance Management\",\n",
    "    \"Employee Recognition Programs\",\n",
    "    \"Communication Channels\",\n",
    "    \"Workforce Planning\",\n",
    "    \"Diversity and Inclusion Initiatives\",\n",
    "    \"Conflict Resolution Procedures\",\n",
    "    \"Employee Development Programs\",\n",
    "    \"Change Management Strategies\",\n",
    "    \"Succession Planning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker to generate fake data\n",
    "fake = Faker()\n",
    "\n",
    "num_entries = len(topics)\n",
    "\n",
    "data = {\n",
    "    'rank': np.arange(1, num_entries + 1),\n",
    "    'title': topics,\n",
    "    'publication_date': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_entries)],\n",
    "    'author': [fake.name() for _ in range(num_entries)],  # Changed 'inventor' to 'author'\n",
    "    'department': [fake.company() for _ in range(num_entries)],  # Changed 'organization' to 'department'\n",
    "    'publication_number': [str(fake.random_number(digits=10)) for _ in range(num_entries)],  # Changed 'publication_number' to 'document_number'\n",
    "    'language': ['en'] * num_entries,\n",
    "    'thumbnail': [fake.image_url() for _ in range(num_entries)],\n",
    "    'pdf': [fake.url() for _ in range(num_entries)],\n",
    "    'page': [random.randint(1, 10) for _ in range(num_entries)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc text for ID 8309030038 saved successfully.\n",
      "Doc text for ID 6320554784 saved successfully.\n",
      "Doc text for ID 979711036 saved successfully.\n",
      "Doc text for ID 5644000230 saved successfully.\n",
      "Doc text for ID 2910177507 saved successfully.\n",
      "Doc text for ID 7967943304 saved successfully.\n",
      "Doc text for ID 9825861125 saved successfully.\n",
      "Doc text for ID 9058286412 saved successfully.\n",
      "Doc text for ID 5632315866 saved successfully.\n",
      "Doc text for ID 5346207284 saved successfully.\n",
      "Doc text for ID 2640696840 saved successfully.\n",
      "Doc text for ID 3360395245 saved successfully.\n",
      "Doc text for ID 4406538911 saved successfully.\n",
      "Doc text for ID 5741425636 saved successfully.\n",
      "Doc text for ID 7297106292 saved successfully.\n",
      "Doc text for ID 9047011157 saved successfully.\n",
      "Doc text for ID 1770017717 saved successfully.\n",
      "Doc text for ID 690856158 saved successfully.\n",
      "Doc text for ID 5563506326 saved successfully.\n",
      "Doc text for ID 2007163696 saved successfully.\n",
      "All Docs texts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Function to generate synthetic patent text\n",
    "def generate_patent_text(title):\n",
    "    prompt = f\"Create a text to simulate a adminstrative document within a company on \\\"{title}\\\"\"\n",
    "    deployment_name = \"gpt-35-turbo-instruct\"\n",
    "    response = client.completions.create(\n",
    "        model=deployment_name,\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Function to save synthetic patent text to file\n",
    "def save_patent_text_to_file(id, text):\n",
    "    directory = \"synthetic_admin\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(os.path.join(directory, f\"{id.replace(\"/\", \"_\")}.txt\"), \"w\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "# Generate synthetic patent texts and save them to files\n",
    "for index, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    patent_text = generate_patent_text(title)\n",
    "    save_patent_text_to_file(row['publication_number'], patent_text)\n",
    "    print(f\"Doc text for ID {row['publication_number']} saved successfully.\")\n",
    "\n",
    "print(\"All Docs texts saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list to store text data\n",
    "text_data = []\n",
    "\n",
    "# Function to read text files and return text content\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()\n",
    "    \n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Read the corresponding text file\n",
    "    file_path = f\"synthetic_admin/{row['publication_number'].replace(\"/\", \"_\")}.txt\"\n",
    "    if os.path.exists(file_path):\n",
    "        text_content = read_text_file(file_path)\n",
    "        text_data.append({\"publication_number\": row[\"publication_number\"], \"text\": text_content})\n",
    "\n",
    "# Create a DataFrame from the text data\n",
    "text_df = pd.DataFrame(text_data)\n",
    "\n",
    "# Join the text DataFrame with the original DataFrame on the \"id\" column\n",
    "merged_df = pd.merge(df, text_df, on=\"publication_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_char = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    "    separators=[\"<ele>\", \". \", \" \"],\n",
    "    keep_separator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def chunk_documents(content_df: pd.DataFrame, text_splitter, word_count_threshold: int = 10, element_separator: str = \"<ele>\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into chunks according to the configuration of the text splitter\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Split content into chunks\n",
    "    content_df[\"text_chunks\"] = content_df[\"text\"].apply(text_splitter.split_text)\n",
    "    content_df = content_df.explode(\"text_chunks\")\n",
    "    content_df[\"text_chunks\"] = content_df[\"text_chunks\"].str.replace(element_separator, \", \")\n",
    "    \n",
    "    content_df[\"text_chunks\"] = content_df[\"title\"] + \": \" + content_df[\"text_chunks\"]\n",
    "    \n",
    "    # Drop failed extractions\n",
    "    print(f'Files with extraction errors: {set(content_df[content_df[\"text_chunks\"].isna()][\"title\"].to_list())}')\n",
    "    content_df = content_df.dropna(axis=0, subset=[\"text_chunks\"])\n",
    "    \n",
    "    # Drop chunks with less words than the threshold allows\n",
    "    content_df = content_df[content_df[\"text_chunks\"].apply(lambda n: len(n.split())) > word_count_threshold]\n",
    "\n",
    "    return content_df\n",
    "\n",
    "# Example usage:\n",
    "# chunked_df = chunk_documents(df, text_splitter_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with extraction errors: set()\n"
     ]
    }
   ],
   "source": [
    "chunk_df = chunk_documents(merged_df, text_splitter_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Few-shot prompt for entity extraction\n",
    "few_shot_prompt = \"\"\"\n",
    "RETURN ONLY A VALID JSON OBJECT!!! NO ADDITIONAL TEXT\n",
    "\n",
    "Extract relevant entities such as departments, procedures, technologies, and projects from the given administrative document.\n",
    "\n",
    "Example 1:\n",
    "Text: \"The new HR management system enhances employee onboarding processes.\"\n",
    "{\"departments\": [], \"procedures\": [\"employee onboarding processes\"], \"technologies\": [\"HR management system\"], \"projects\": []}\n",
    "\n",
    "Example 2:\n",
    "Text: \"The safety procedures were updated for the manufacturing department.\"\n",
    "{\"departments\": [\"manufacturing\"], \"procedures\": [\"safety procedures\"], \"technologies\": [], \"projects\": []}\n",
    "\n",
    "Example 3:\n",
    "Text: \"The IT department implemented a new cybersecurity protocol.\"\n",
    "{\"departments\": [\"IT\"], \"procedures\": [\"cybersecurity protocol\"], \"technologies\": [], \"projects\": []}\n",
    "\n",
    "Example 4:\n",
    "Text: \"The company ABC Inc. initiated a project for workplace diversity training.\"\n",
    "{\"departments\": [], \"procedures\": [], \"technologies\": [], \"projects\": [\"workplace diversity training\"]}\n",
    "\n",
    "Example 5:\n",
    "Text: \"The administrative policies were reviewed and updated.\"\n",
    "{\"departments\": [], \"procedures\": [\"administrative policies\"], \"technologies\": [], \"projects\": []}\n",
    "\n",
    "RETURN ONLY A VALID JSON OBJECT!!! NO ADDITIONAL TEXT. JUST A JSON\n",
    "IF THERE ARE NO ENTITIES RETURN AN EMPTY JSON\n",
    "---\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract entities from text using Azure OpenAI API\n",
    "def extract_entities(text):\n",
    "    prompt = f\"{few_shot_prompt}\\nText: \\\"{text}\\\"\\n\"\n",
    "    deployment_name = \"gpt-35-turbo-instruct\"\n",
    "    response = client.completions.create(\n",
    "        model=deployment_name,\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "    # Parse the response as JSON\n",
    "    entities_json = response.choices[0].text.strip()\n",
    "    return entities_json\n",
    "\n",
    "\n",
    "# Create an empty list to store extracted entity data\n",
    "entity_data = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in chunk_df.iterrows():\n",
    "    text = row[\"text_chunks\"]\n",
    "    # Extract entities from the text\n",
    "    entities = extract_entities(text)\n",
    "    if entities is not None:\n",
    "        entity_data.append(entities)\n",
    "    else:\n",
    "        entity_data.append('')\n",
    "\n",
    "\n",
    "chunk_df[\"entities\"] = entity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retriever_model_inference(text_input: list[str]):\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=\"2024-03-01-preview\"\n",
    "        )\n",
    "\n",
    "    embeddings = client.embeddings.create(\n",
    "        input=text_input,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    vectors = [np.array(i.embedding, dtype='f') for i in embeddings.data]\n",
    "\n",
    "\n",
    "    return vectors\n",
    "\n",
    "\n",
    "chunk_df[\"embeddings\"] = retriever_model_inference(chunk_df.text_chunks.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index initialized\n",
      "Shape of input vectors: (143, 1536)\n",
      "Index trained\n",
      "Vectors added to index\n"
     ]
    }
   ],
   "source": [
    "vector_dim=1536\n",
    "partition_nr=10\n",
    "\n",
    "from utils.faiss_storage import index_initializaton, train_index, add_index_vectors\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. initialize index\n",
    "#index = index_initializaton(vector_dim, partition_nr)\n",
    "path_to_index = \"a4_db_onboarding.index\"\n",
    "\n",
    "def check_index_state(index):\n",
    "    return index.is_trained\n",
    "\n",
    "def vector_db_setup(\n",
    "    vector_dim: int, partition_nr: int, df: pd.DataFrame\n",
    "):\n",
    "    # 1. initialize index\n",
    "    index = index_initializaton(vector_dim, partition_nr)\n",
    "    print(\"Index initialized\")\n",
    "\n",
    "    # 2. check index state - and train if required\n",
    "    if check_index_state(index):\n",
    "        pass\n",
    "    else:\n",
    "        vectors = df[\"embeddings\"].values\n",
    "        vectors = np.array([i.astype(np.float32) for i in chunk_df[\"embeddings\"].values])\n",
    "        print(\"Shape of input vectors:\", vectors.shape)\n",
    "        train_index(index, vectors)\n",
    "        print(\"Index trained\")\n",
    "        # 3. add vectors to index\n",
    "        add_index_vectors(index, vectors)\n",
    "        print(\"Vectors added to index\")\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "index = vector_db_setup(vector_dim, partition_nr, chunk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, path_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = faiss.read_index(path_to_index)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.drop(columns=[\"embeddings\"]).to_csv(\"a4_db_onboarding.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"departments\": [], \"procedures\": [\"Administrative Policies and Procedures\", \"administrative policies and procedures\", \"office hours\"], \"technologies\": [], \"projects\": []}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.read_csv(\"a4_db_onboarding.csv\").head(3).entities.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
